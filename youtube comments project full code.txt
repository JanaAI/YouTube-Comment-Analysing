from googleapiclient.discovery import build
import pandas as pd
import seaborn as sns

#get channel id & own api key
api_key = 'AIzaSyDemrEPlblsgUbTOX6ENZZXu0ZaSHjSxYc'
channel_id = ['UCM-yUTYGmrNvKOCcAl21g3w',     #CSIT programming
              'UC-JFyL0zDFOsPMpuWu39rPA',     #Thanthi News
              'UCiT9RITQ9PW6BhXK0y2jaeg',     #Ken Jee
              'UCnz-ZXXER4jOvuED5trXfEA'      #Tech TFQ
              ]

youtube = build('youtube', 'v3', developerKey=api_key)

#extract channel details
#function to get channel statistics
def get_channel_stats(youtube, channel_id):
  all_data = []
  request = youtube.channels(). list(
      part='snippet,contentDetails,statistics',
      id= ','.join(channel_id))
  response = request.execute()
  for i in range(len(response['items'])):
      data = dict (Channel_name = response['items'] [i] ['snippet'] ['title'],
                  Subscribers = response['items'] [i] ['statistics'] ['subscriberCount'],
                  Views = response['items'] [i] ['statistics'] ['viewCount'],
                  Total_videos = response['items'] [i] ['statistics'] ['videoCount'],
                  playlist_id = response['items'] [i] ['contentDetails'] ['relatedPlaylists'] ['uploads'])
      all_data.append(data)


  return all_data

channel_statistic = get_channel_stats(youtube, channel_id)

channel_data = pd.DataFrame(channel_statistic)

channel_data

channel_data['Subscribers'] = pd.to_numeric(channel_data['Subscribers'])
channel_data['Views'] = pd.to_numeric(channel_data['Views'])
channel_data['Total_videos'] = pd.to_numeric(channel_data['Total_videos'])
channel_data.dtypes

#subscriber count
sns.set(rc={'figure.figsize':(7,7)})
ax = sns.barplot(x='Channel_name', y='Subscribers', data=channel_data)

#viewer count
ax = sns.barplot(x='Channel_name', y='Views', data=channel_data)

#total videos
ax = sns.barplot(x='Channel_name', y='Total_videos', data=channel_data)

channel_data

playlist_id = channel_data.loc[channel_data['Channel_name']=='Ken Jee', 'playlist_id'].iloc[0]

playlist_id

# get video id
def get_video_id(youtube, playlist_id):
  request = youtube.playlistItems().list(
      part='contentDetails',
      playlistId = playlist_id,
      maxResults = 50)
  response = request.execute()

  video_ids = []

  for i in range(len(response['items'])):
    video_ids.append(response['items'] [i] ['contentDetails']['videoId'])

  next_page_token = response.get('nextPageToken')
  more_pages = True

  while more_pages:
    if next_page_token is None:
       more_pages  = False
    else:
      request = youtube.playlistItems().list(
             part='contentDetails',
             playlistId = playlist_id,
             maxResults = 50,
             pageToken = next_page_token)
      response = request.execute()

      for i in range(len(response['items'])):
          video_ids.append(response['items'] [i] ['contentDetails']['videoId'])

      next_page_token = response.get('nextPageToken')


  return video_ids

video_ids = get_video_id(youtube, playlist_id)

video_ids

#get video details
def get_video_details(youtube, video_ids):
  all_video_stats = []

  for i in range(0, len(video_ids), 50):
    request = youtube.videos().list(
                part='snippet,statistics',
                id=','.join(video_ids[i:i+50]))
    response = request.execute()

    for video in response['items']:
      video_stats = dict(Title = video['snippet']['title'],
                         Published_date = video['snippet']['publishedAt'],
                         Views = video['statistics']['viewCount'],
                         Likes = video['statistics']['likeCount'],
                         favorite = video['statistics']['favoriteCount'],
                         Comments = video['statistics']['commentCount']
                        )
      all_video_stats.append(video_stats)

    return all_video_stats
video_details = get_video_details(youtube, video_ids)

video_data = pd.DataFrame(video_details)

video_data['Published_date'] = pd.to_datetime(video_data['Published_date']).dt.date
video_data['Views'] = pd.to_numeric(video_data['Views'])
video_data['Likes'] = pd.to_numeric(video_data['Likes'])
video_data['Views'] = pd.to_numeric(video_data['Views'])
video_data

top10_videos = video_data.sort_values(by='Views',ascending=False)

top10_videos

ax1 = sns.barplot(x='Views', y='Title', data=top10_videos)

video_data

video_data['Month'] = pd.to_datetime(video_data['Published_date']).dt.strftime('%b')

videos_per_month = video_data.groupby('Month',as_index=False).size()

video_data

sort_order = ['Jan','Feb','Mar','Apr','May','Jun'
              'Jul','Aug','Sept','Oct','Nov','Dec']

videos_per_month.index = pd.CategoricalIndex(videos_per_month['Month'],categories=sort_order , ordered=True)

videos_per_month = videos_per_month.sort_index()

ax2 = sns.barplot(x='Month', y='size', data=videos_per_month)

video_data.to_csv('Video_Details(Ken Jee).csv')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import os

# Import functions for data preprocessing & data preparation
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
from sklearn.feature_extraction.text import CountVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer, LancasterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords
from nltk.corpus import wordnet
import string
from string import punctuation
import nltk
import re

data = pd.read_csv('/content/Ken Jee comment dataset.csv',encoding='utf-8',error_bad_lines=False);
data.columns
data1=data.drop(['Name','Likes','Time','Published','Updated'],axis=1)
data1

!pip install emoji
!pip install vaderSentiment
!pip install goolge-api-python-client

# For Fetching Comments
from googleapiclient.discovery import build
# For filtering comments
import re
# For filtering comments with just emojis
import emoji
# Analyze the sentiments of the comment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# For visualization
import matplotlib.pyplot as plt

API_KEY = 'AIzaSyDemrEPlblsgUbTOX6ENZZXu0ZaSHjSxYc '# Put in your API Key

youtube = build('youtube', 'v3', developerKey=API_KEY) # initializing Youtube API

# Taking input from the user and slicing for video id
video_id = input('Enter Youtube Video URL: ')[-11:]
print("video id: " + video_id)

# Getting the channelId of the video uploader
video_response = youtube.videos().list(
	part='snippet',
	id=video_id
).execute()

# Splitting the response for channelID
video_snippet = video_response['items'][0]['snippet']
uploader_channel_id = video_snippet['channelId']
print("channel id: " + uploader_channel_id)


# Fetch comments
print("Fetching Comments...")
comments = []
nextPageToken = None
while len(comments) < 600:
	request = youtube.commentThreads().list(
		part='snippet',
		videoId=video_id,
		maxResults=100, # You can fetch up to 100 comments per request
		pageToken=nextPageToken
	)
	response = request.execute()
	for item in response['items']:
		comment = item['snippet']['topLevelComment']['snippet']
		# Check if the comment is not from the video uploader
		if comment['authorChannelId']['value'] != uploader_channel_id:
			comments.append(comment['textDisplay'])
	nextPageToken = response.get('nextPageToken')

	if not nextPageToken:
		break
# Print the 5 comments
comments[:5]

# Fetch comments
print("Fetching Comments...")
comments = []
nextPageToken = None
while len(comments) < 600:
	request = youtube.commentThreads().list(
		part='snippet',
		videoId=video_id,
		maxResults=100, # You can fetch up to 100 comments per request
		pageToken=nextPageToken
	)
	response = request.execute()
	for item in response['items']:
		comment = item['snippet']['topLevelComment']['snippet']
		# Check if the comment is not from the video uploader
		if comment['authorChannelId']['value'] != uploader_channel_id:
			comments.append(comment['textDisplay'])
	nextPageToken = response.get('nextPageToken')

	if not nextPageToken:
		break
# Print the 5 comments
comments[:5]

hyperlink_pattern = re.compile(
	r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')

threshold_ratio = 0.65

relevant_comments = []

# Inside your loop that processes comments
for comment_text in comments:

	comment_text = comment_text.lower().strip()

	emojis = emoji.emoji_count(comment_text)

	# Count text characters (excluding spaces)
	text_characters = len(re.sub(r'\s', '', comment_text))

	if (any(char.isalnum() for char in comment_text)) and not hyperlink_pattern.search(comment_text):
		if emojis == 0 or (text_characters / (text_characters + emojis)) > threshold_ratio:
			relevant_comments.append(comment_text)

# Print the relevant comments
relevant_comments[:5]

f = open("ytcomments.txt", 'w', encoding='utf-8')
for idx, comment in enumerate(relevant_comments):
	f.write(str(comment)+"\n")
f.close()
print("Comments stored successfully!")


def sentiment_scores(comment, polarity):

	# Creating a SentimentIntensityAnalyzer object.
	sentiment_object = SentimentIntensityAnalyzer()

	sentiment_dict = sentiment_object.polarity_scores(comment)
	polarity.append(sentiment_dict['compound'])

	return polarity


polarity = []
positive_comments = []
negative_comments = []
neutral_comments = []

f = open("ytcomments.txt", 'r', encoding='`utf-8')
comments = f.readlines()
f.close()
print("Analysing Comments...")
for index, items in enumerate(comments):
	polarity = sentiment_scores(items, polarity)

	if polarity[-1] > 0.05:
		positive_comments.append(items)
	elif polarity[-1] < -0.05:
		negative_comments.append(items)
	else:
		neutral_comments.append(items)

# Print polarity
polarity[:5]

avg_polarity = sum(polarity)/len(polarity)
print("Average Polarity:", avg_polarity)
if avg_polarity > 0.05:
	print("The Video has got a Positive response")
elif avg_polarity < -0.05:
	print("The Video has got a Negative response")
else:
	print("The Video has got a Neutral response")

print("The comment with most positive sentiment:", comments[polarity.index(max(
	polarity))], "with score", max(polarity), "and length", len(comments[polarity.index(max(polarity))]))
print("The comment with most negative sentiment:", comments[polarity.index(min(
	polarity))], "with score", min(polarity), "and length", len(comments[polarity.index(min(polarity))]))

positive_count = len(positive_comments)
negative_count = len(negative_comments)
neutral_count = len(neutral_comments)

# labels and data for Bar chart
labels = ['Positive', 'Negative', 'Neutral']
comment_counts = [positive_count, negative_count, neutral_count]

# Creating bar chart
plt.bar(labels, comment_counts, color=['blue', 'red', 'grey'])

# Adding labels and title to the plot
plt.xlabel('Sentiment')
plt.ylabel('Comment Count')
plt.title('Sentiment Analysis of Comments')

# Displaying the chart
plt.show()




